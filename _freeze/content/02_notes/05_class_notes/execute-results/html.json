{
  "hash": "1919a2f89c580a9158738bef8990dcf7",
  "result": {
    "markdown": "---\ntitle: \"Class Notes\"\nauthor: \"Amin Raza\"\n---\n\n\nMy original code for the LIME challenge:\n\n\n::: {.cell hash='05_class_notes_cache/html/unnamed-chunk-1_036bc2186c3e91f4ecb2947af064e255'}\n\n```{.r .cell-code}\n# LIME FEATURE EXPLANATION ----\n\n# 1. Setup ----\n\n# Load Libraries \n\nlibrary(h2o)\nlibrary(recipes)\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(lime)\nlibrary(rsample)\n\n\n# Load Data\nemployee_attrition_tbl <- read_csv(\"datasets-1067-1925-WA_Fn-UseC_-HR-Employee-Attrition.csv\")\ndefinitions_raw_tbl    <- read_excel(\"data_definitions.xlsx\", sheet = 1, col_names = FALSE)\n\n#Source Code was not given, lets hope I can take the table from the H2O Business Case\n\nemployee_attrition_readable_tbl <- readRDS(\"employee_attrition_tbl.rds\")\n\n# Split into test and train\nset.seed(seed = 1113)\nsplit_obj <- rsample::initial_split(employee_attrition_readable_tbl, prop = 0.85)\n\n# Assign training and test data\ntrain_readable_tbl <- training(split_obj)\ntest_readable_tbl  <- testing(split_obj)\n\n# ML Preprocessing Recipe \nrecipe_obj <- recipe(Attrition ~ ., data = train_readable_tbl) %>%\n  step_zv(all_predictors()) %>%\n  step_mutate_at(c(\"JobLevel\", \"StockOptionLevel\"), fn = as.factor) %>% \n  prep()\n\nrecipe_obj\n\ntrain_tbl <- bake(recipe_obj, new_data = train_readable_tbl)\ntest_tbl  <- bake(recipe_obj, new_data = test_readable_tbl)\n\n# 2. Models ----\n\nh2o.init()\n\n#we calculate a new model \nsplit_h2o <- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.85), seed = 1234)\ntrain_h2o <- split_h2o[[1]]\nvalid_h2o <- split_h2o[[2]]\ntest_h2o  <- as.h2o(test_tbl)\n\n# Set the target and predictors\ny <- \"Attrition\"\nx <- setdiff(names(train_h2o), y)\n\n#Compute the models\n\nautoml_models_h2o <- h2o.automl(\n  x = x,\n  y = y,\n  training_frame    = train_h2o,\n  validation_frame  = valid_h2o,\n  leaderboard_frame = test_h2o,\n  max_runtime_secs  = 30,\n  nfolds            = 5 \n)\n\nautoml_models_h2o@leaderboard\n\n#Take the best model from the calculation\n\nautoml_leader <-h2o.getModel(\"StackedEnsemble_AllModels_2_AutoML_10_20230611_191016\")\nautoml_leader\n\n\n# 3. LIME ----\n\n# 3.1 Making Predictions ----\n\npredictions_tbl <- automl_leader %>% \n  h2o.predict(newdata = as.h2o(test_tbl)) %>%\n  as.tibble() %>%\n  bind_cols(\n    test_tbl %>%\n      select(Attrition, EmployeeNumber)\n  )\n\npredictions_tbl\n\n#Letâ€™s investigate the 1st employee, that did indeed leave the company:\ntest_tbl %>%\n  slice(1) %>%\n  glimpse()\n\n\n\n# 3.2 Single Explanation ----\n\nexplainer <- train_tbl %>%\n  select(-Attrition) %>%\n  lime(\n    model           = automl_leader,\n    bin_continuous  = TRUE,\n    n_bins          = 4,\n    quantile_bins   = TRUE\n  )\n\nexplainer\n\n\nexplanation_single <- test_tbl %>%\n  slice(1) %>%\n  select(-Attrition) %>%\n  lime::explain(\n    \n    # Pass our explainer object\n    explainer = explainer,\n    # Because it is a binary classification model: 1\n    n_labels   = 1,\n    # number of features to be returned\n    n_features = 8,\n    # number of localized linear models\n    n_permutations = 5000,\n    # Let's start with 1\n    kernel_width   = 1\n  )\n\nexplanation\n\nexplanation %>%\n  as.tibble() %>%\n  select(feature:prediction) \n\nplot_features(explanation = explanation_single, ncol = 1)\n\n\n\n# 3.3 Multiple Explanations ----\n\nexplanation_multiple <- test_tbl %>%\n  slice(1:20) %>%\n  select(-Attrition) %>%\n  lime::explain(\n    explainer = explainer,\n    n_labels   = 1,\n    n_features = 8,\n    n_permutations = 5000,\n    kernel_width   = 0.5\n  )\n\nexplanation_multiple %>%\n  as.tibble()\n\n#Messy plot\nplot_features(explanation, ncol = 4)\n\n#Still hard to read\nplot_explanations(explanation_multiple)\n\n\n\n######CHALLENGE\nexplanation_single %>% \n  as.tibble()\n\ncase_1 <- explanation_single %>%\n  filter(case == 1)\n\ncase_1 %>%\n  plot_features()\n\n###Recreate the plot above\n###Part 1\n\n#Step 1 choose relevant columns\n#Create a new column which indicates the sign of the value feature weight to color it later respectivly \nreplica1_tbl <- case_1 %>%\n  select(feature_weight, feature_desc, case, label_prob) %>%\n  mutate(sign =  ifelse(feature_weight >= 0, \"Supports\", \"Contradiction\")) %>%\n  arrange(desc(abs(feature_weight))) \n\n\n#plot\nggplot(data=replica1_tbl, aes(reorder(feature_desc, abs(feature_weight), sum), feature_weight, fill = sign)) +\n  geom_col() +\n  scale_fill_manual(values = c(\"Supports\" = \"#4983B2\", \"Contradiction\" = \"#B02427\")) +\n  coord_flip() +\n  labs(y= \"Weight\", x = \"Feature\") +\n  theme(legend.position = \"bottom\") +\n  guides(fill=guide_legend(title=\"\")) +\n  ggtitle(\" Case: 1\\n Label: No\\n Probability: 0.67\\n Explanation Fit: 0.35\")\n\n###Part 2 \n#Recreate the plot_explanations()\n\n\n\nplot_explanations(explanation_multiple)\n\n#explanation_multiple$feature_desc <- factor(\n#  explanation_multiple$feature_desc,\n#  levels = rev(unique(explanation_multiple$feature_desc[order(explanation_multiple$feature, explanation_multiple$feature_value)]))\n#)\n\n\n#Essentially I think I do the same as the code above but in a more specific way adapted to the data I work with\nexplanation_multiple$case <- factor(explanation_multiple$case,levels = c(\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\"))\n\n\n#I copied a lot from the github code because I really didnt have a clue\n\nggplot(explanation_multiple, aes(case, feature_desc)) +\n  geom_tile(aes(fill = feature_weight)) +\n  scale_x_discrete('Case', expand = c(0, 0)) +\n  scale_y_discrete('Feature', expand = c(0, 0)) +\n  scale_fill_gradient2('Feature\\nweight', low = 'firebrick', mid = '#f7f7f7', high = 'steelblue') +\n  theme(panel.border = element_rect(fill = NA, colour = 'grey60', linewidth =  1),\n        panel.grid = element_blank(),\n        legend.position = 'right',\n        axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +\n  facet_wrap(~label) +\n  theme(legend.background = element_blank(), panel.background = element_blank(),axis.ticks = element_blank())\n\n\n#Close enough!\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}