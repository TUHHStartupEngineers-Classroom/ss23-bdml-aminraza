{
  "hash": "547a31784c0bfbe8890b57a9cff7ddb5",
  "result": {
    "markdown": "---\ntitle: \"Supervised Machine Learning - Regression\"\nauthor: \"Amin Raza\"\n\noutput: \n    html_document:\n        code_folding: hide\n---\n\n\n\n\n# Challenge\n\nChallenge information taken from the course Website.\n\n## Build a model\n\nBecause there were no further instructions we use the model from the business case\n\n\n::: {.cell hash='02_supervised_ML_cache/html/unnamed-chunk-1_debea3e70b0523c8adbf32cb6ef86b91'}\n\n```{.r .cell-code}\n#Step 1 - Build a Model\n\n\nlibrary(recipes)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(rsample)\nlibrary(workflows)\nlibrary(parsnip)\n\n\nbike_features_tbl <- readRDS(\"02_supervised_ML_files/bike_features_tbl.rds\") %>%\n select(model, price, frame_material, weight) \n```\n:::\n\n\n## Create features with the recipes package\n\nAs predictors I choose the weight. The outcome shall be the price of the bike.\nI know that weight alone is only a limited factor in predicting a prize, \nbut as I understand it, the Challenge is all about the principle.\n\n\n::: {.cell hash='02_supervised_ML_cache/html/unnamed-chunk-2_f8a445262b6da940cf2f11cde1911697'}\n\n```{.r .cell-code}\n# Step 2: Creating Features with recipes package\n\nbike_recipe <- recipe(price ~ weight, data = bike_features_tbl) %>%\n  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) #%>%\n  #step_rename(aluminium = frame_material_aluminium) %>%\n  #step_rename(carbon = frame_material_carbon)\n\n#Apply recipe and bake\n\nbike_baked <- prep(bike_recipe) %>%\n  bake(new_data = bike_features_tbl) %>%\n  mutate(model = bike_features_tbl$model)\n\n#Plot the data we have\n\nggplot(bike_baked, aes(x = price, y = weight)) +\n  geom_point() +\n  labs(title = \"Scatter Plot\", x = \"Price\", y = \"Weight\")\n```\n\n::: {.cell-output-display}\n![](02_supervised_ML_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Step 3: Splitting into Training and Test Sets\nset.seed(1113)\nsplit_obj <- initial_split(bike_baked, prop = 0.80)\n\ntrain_tbl <- training(split_obj)\ntest_tbl <- testing(split_obj)\n```\n:::\n\nIn the scatter plot you can see that there is no linear relation. \nTherefore, I calculate not only a linear regression model but also a descision tree and a random forrest model (just because I am curious what the outcome is).\n\n## Bundle the model and recipe with the workflow package\n\n\n::: {.cell hash='02_supervised_ML_cache/html/unnamed-chunk-3_b446c1bb4a4c3d0a56b9732c2e36eddf'}\n\n```{.r .cell-code}\n#Bundle the model and recipe with the workflow package\n#linear regression model\nmodel_01_linear_lm_simple <- linear_reg(mode = \"regression\")\n\n\n#Create a workflow object using the workflow() function. \n#Serves as a container for your model and recipe:\n#Model linear regression\n\nworkflow_obj <- workflow(bike_recipe, model_01_linear_lm_simple) %>%\nfit(data = train_tbl)\n\nmy_prediction <- predict(workflow_obj, new_data = test_tbl)\n\ncomparison <- my_prediction %>%\n  mutate(weight = test_tbl$weight) %>%\n  mutate(correct_price = test_tbl$price)\n\n\n#same for decision tree\n#decission tree model\nmodel_04_tree_decision_tree <- decision_tree(mode = \"regression\",\n                                             \n                                             # Set the values accordingly to get started\n                                             cost_complexity = 0.001,\n                                             tree_depth      = 5,\n                                             min_n           = 7)\n\nworkflow_obj_tree <- workflow(bike_recipe, model_04_tree_decision_tree) %>%\n  fit(data = train_tbl)\n\nmy_prediction_tree <- predict(workflow_obj_tree, new_data = test_tbl)\n\ncomparison_tree <- my_prediction_tree%>%\n  mutate(weight = test_tbl$weight) %>%\n  mutate(correct_price = test_tbl$price)\n\n#random forest\nmodel_05_rand_forest <- rand_forest(\n  mode = \"regression\", mtry = 8, trees = 5000, min_n = 10\n)\n\nworkflow_obj_forest <- workflow(bike_recipe, model_05_rand_forest) %>%\n  fit(data = train_tbl)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning: 8 columns were requested but there were 1 predictors in the data. 1\n#> will be used.\n```\n:::\n\n```{.r .cell-code}\nmy_prediction_forest <- predict(workflow_obj_forest, new_data = test_tbl)\n\ncomparison_forest <- my_prediction_forest%>%\n  mutate(weight = test_tbl$weight) %>%\n  mutate(correct_price = test_tbl$price)\n```\n:::\n\n\n## Evaluate your model with the yardstick package\n\nI evaluate all 3 models.\n\n\n::: {.cell hash='02_supervised_ML_cache/html/unnamed-chunk-4_9d2abd15494c35c8a087ac376a6a4e99'}\n\n```{.r .cell-code}\n#Evalute models\ncomparison %>%\nyardstick::metrics(truth = correct_price, estimate = .pred)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\".metric\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimator\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimate\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"rmse\",\"2\":\"standard\",\"3\":\"1.617320e+03\"},{\"1\":\"rsq\",\"2\":\"standard\",\"3\":\"6.479918e-03\"},{\"1\":\"mae\",\"2\":\"standard\",\"3\":\"1.330425e+03\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\ncomparison_tree %>%\n  yardstick::metrics(truth = correct_price, estimate = .pred)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\".metric\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimator\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimate\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"rmse\",\"2\":\"standard\",\"3\":\"1316.184042\"},{\"1\":\"rsq\",\"2\":\"standard\",\"3\":\"0.349141\"},{\"1\":\"mae\",\"2\":\"standard\",\"3\":\"1075.148481\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\ncomparison_forest %>%\n  yardstick::metrics(truth = correct_price, estimate = .pred)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\".metric\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimator\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimate\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"rmse\",\"2\":\"standard\",\"3\":\"1274.3694114\"},{\"1\":\"rsq\",\"2\":\"standard\",\"3\":\"0.3763181\"},{\"1\":\"mae\",\"2\":\"standard\",\"3\":\"1071.4818898\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nIf we just look at metric RMSE, the random forest model performs the best (RMSE = 1274,37).\n\nAs I mentioned in the beginning you could see on the scatter plot, \nthat there is no linear relation recognizable. Therefore it is no wonder, that\nthe linear regression model performs worst (RMSE = 1617,32).\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}