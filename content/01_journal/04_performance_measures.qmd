---
title: "Performance Measures"
author: "Amin Raza"

output: 
    html_document:
        code_folding: hide
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(
    echo = TRUE,
    message = FALSE,
    warning = FALSE
    )
```

```{r, message=FALSE}
library(tidyverse)
library(readxl)
library(h2o)
library(dplyr)
library(purrr)
library(rsample)
library(recipes)
```

I had difficulties calculating models within the enviroment of Quarto. That's why I executed the code in another independent R script and just exported the plots and the tables. The code however is shown here for the purpose of evaluation.

If something is not clear, please look at the comments I made in the code. I tried to document my process there.

# Challenge

We perform the steps from the last challenge to obtain a leaderboard to work with.

```{r, eval = FALSE}

#load data
product_backorders_tbl <- read_csv("product_backorders.csv")

#Split into training and data set
set.seed(seed = 1113)
split_obj <- rsample::initial_split(product_backorders_tbl, prop = 0.85)

train_tbl <- training(split_obj)
test_tbl  <- testing(split_obj)

#Prepare recipe and choose predictors 
recipe_obj <- recipe(went_on_backorder ~., data = train_tbl) %>% 
  prep()

train_tbl <- bake(recipe_obj, new_data = train_tbl)
test_tbl  <- bake(recipe_obj, new_data = test_tbl)
```

We calculate the models with the code below. Again I this code won't be executed, the models I used are already calculated.

```{r, eval= FALSE}
# Modeling
h2o.init()

# Split data into a training and a validation data frame
# Setting the seed is just for reproducability
split_h2o <- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.85), seed = 1234)
train_h2o <- split_h2o[[1]]
valid_h2o <- split_h2o[[2]]
test_h2o  <- as.h2o(test_tbl)

# Set the target and predictors
y <- "went_on_backorder"
x <- setdiff(names(train_h2o), y)

#Compute the models

automl_models_h2o <- h2o.automl(
  x = x,
  y = y,
  training_frame    = train_h2o,
  validation_frame  = valid_h2o,
  leaderboard_frame = test_h2o,
  max_runtime_secs  = 60,
  nfolds            = 5 
)
```

## Leaderboard Visualization

```{r, eval=FALSE}
####LEADERBOARD VISUALIZATION###
leaderboard_sorted <- automl_models_h2o@leaderboard %>% 
  as_tibble() %>% 
  select(-c(mean_per_class_error, rmse, mse))
```

Here is the leaderboard from the preceded calculation.

```{r}
read_csv("04_performance_measures_files/leaderboard_sorted.csv")
```

```{r, eval=FALSE}
# Visualize the H2O leaderboard to help with model selection
data_transformed_tbl <- automl_models_h2o@leaderboard %>%
  as_tibble() %>%
  select(-c(aucpr, mean_per_class_error, rmse, mse)) %>% 
  mutate(model_type = str_extract(model_id, "[^_]+")) %>%
  slice(1:15) %>% 
  rownames_to_column(var = "rowname") %>%
  # Visually this step will not change anything
  # It reorders the factors under the hood
  mutate(
    model_id   = as_factor(model_id) %>% reorder(auc),
    model_type = as.factor(model_type)
  ) %>% 
  pivot_longer(cols = -c(model_id, model_type, rowname), 
               names_to = "key", 
               values_to = "value", 
               names_transform = list(key = forcats::fct_inorder)
  ) %>% 
  mutate(model_id = paste0(rowname, ". ", model_id) %>% as_factor() %>% fct_rev())

```

The table looks like this.

```{r}
read_csv("04_performance_measures_files/calculated_gain_lift_tbl.csv")
```


```{r, eval=FALSE}
#Plot the data
data_transformed_tbl %>%
  ggplot(aes(value, model_id, color = model_type)) +
  geom_point(size = 3) +
  geom_label(aes(label = round(value, 2), hjust = "inward")) +
  
  # Facet to break out logloss and auc
  facet_wrap(~ key, scales = "free_x") +
  labs(title = "Leaderboard Metrics",
       subtitle = paste0("Ordered by: ", "auc"),
       y = "Model Postion, Model ID", x = "") + 
  theme(legend.position = "bottom")
```

And the plot looks as follows:

![Leaderboard Metrics](04_performance_measures_files/data_transformed_tbl_plot.jpg)
I just copied the following code from the business case.

```{r, eval=FALSE}
#Function for what we did above (plotting)
plot_h2o_leaderboard <- function(h2o_leaderboard, order_by = c("auc", "logloss"), 
                                 n_max = 20, size = 4, include_lbl = TRUE) {
  
  # Setup inputs
  # adjust input so that all formats are working
  order_by <- tolower(order_by[[1]])
  
  leaderboard_tbl <- h2o_leaderboard %>%
    as.tibble() %>%
    select(-c(aucpr, mean_per_class_error, rmse, mse)) %>% 
    mutate(model_type = str_extract(model_id, "[^_]+")) %>%
    rownames_to_column(var = "rowname") %>%
    mutate(model_id = paste0(rowname, ". ", model_id) %>% as.factor())
  
  # Transformation
  if (order_by == "auc") {
    
    data_transformed_tbl <- leaderboard_tbl %>%
      slice(1:n_max) %>%
      mutate(
        model_id   = as_factor(model_id) %>% reorder(auc),
        model_type = as.factor(model_type)
      ) %>%
      pivot_longer(cols = -c(model_id, model_type, rowname), 
                   names_to = "key", 
                   values_to = "value", 
                   names_transform = list(key = forcats::fct_inorder)
      )
    
  } else if (order_by == "logloss") {
    
    data_transformed_tbl <- leaderboard_tbl %>%
      slice(1:n_max) %>%
      mutate(
        model_id   = as_factor(model_id) %>% reorder(logloss) %>% fct_rev(),
        model_type = as.factor(model_type)
      ) %>%
      pivot_longer(cols = -c(model_id, model_type, rowname), 
                   names_to = "key", 
                   values_to = "value", 
                   names_transform = list(key = forcats::fct_inorder)
      )
    
  } else {
    # If nothing is supplied
    stop(paste0("order_by = '", order_by, "' is not a permitted option."))
  }
  
  # Visualization
  g <- data_transformed_tbl %>%
    ggplot(aes(value, model_id, color = model_type)) +
    geom_point(size = size) +
    facet_wrap(~ key, scales = "free_x") +
    labs(title = "Leaderboard Metrics",
         subtitle = paste0("Ordered by: ", toupper(order_by)),
         y = "Model Postion, Model ID", x = "")
  
  if (include_lbl) g <- g + geom_label(aes(label = round(value, 2), 
                                           hjust = "inward"))
  
  return(g)
  
}
```

I saved the following models from my calculation to compare later on. 

```{r, eval=FALSE}

#Save some models to compare
#StackedEnsemble
h2o.getModel("StackedEnsemble_BestOfFamily_2_AutoML_7_20230613_130012") %>% 
  h2o.saveModel(path = "h2o_models_new")
#GBM
h2o.getModel("GBM_4_AutoML_7_20230613_130012") %>% 
  h2o.saveModel(path = "h2o_models_new")
#GLM
h2o.getModel("GLM_1_AutoML_7_20230613_130012") %>%
  h2o.saveModel(path = "h2o_models_new")

#load gbm model and predict
gbm_h2o <- h2o.loadModel("/Users/Amin/Documents/GitHub/ss23-bdml-aminraza/content/01_journal/h2o_models_new/GBM_4_AutoML_7_20230613_130012")

predictions <- h2o.predict(gbm_h2o, newdata = as.h2o(test_tbl))

predictions_tbl <- predictions %>% as_tibble()

```

Next we will perform the Grid Search.

## Grid Search

```{r, eval=FALSE}
#####GRID SEARCH#####

#We already did that prior to the prediction, allthough prediction is not relevant yet in this challenge
#stacked_ensemble_h2o <- h2o.loadModel("h20_models/StackedEnsemble_BestOfFamily_2_AutoML_7_20230610_121949")

# Take a look for the metrics on the training data set
# For my model the total error in the confusion matrix is ~4,5 %
gbm_h2o

# We want to see how it performs for the testing data frame
test_tbl

# Make sure to convert it to an h20 object
# Accuracy of the confusion matrix shows ~95 % accuracy
h2o.performance(gbm_h2o, newdata = as.h2o(test_tbl))

?h2o.grid()

gbm_grid_01 <- h2o.grid(
  
  # See help page for available algos
  algorithm = "gbm",
  
  # I just use the same as the object
  grid_id = "gbm_grid_01",
  
  # The following is for ?h2o.deeplearning()
  # predictor and response variables
  x = x,
  y = y,
  
  # training and validation frame and crossfold validation
  training_frame   = train_h2o,
  validation_frame = valid_h2o,
  nfolds = 5,
  
  # Hyperparamters: I use the ones which I can use for GBM
  hyper_params = list(
    # Use some combinations (the first one was the original)
    ntrees = list(c(10)),
    max_depth = c(12,10,8))
)

gbm_grid_01

h2o.getGrid(grid_id = "gbm_grid_01", sort_by = "auc", decreasing = TRUE)

#See how the model performs on test data

gbm_grid_01_model_2 <- h2o.getModel("gbm_grid_01_model_2")

gbm_grid_01_model_2 %>% h2o.auc(train = T, valid = T, xval = T)

# Run it on the test data
gbm_grid_01_model_2 %>%
  h2o.performance(newdata = as.h2o(test_tbl))
# error is ~8.7 % (Confusion matrix)


#Performance of old model
gbm_h2o %>%
  h2o.performance(newdata = as.h2o(test_tbl))
# error is 8,9 %
```

As you could see in my comments in the code, the error of the optimized model is ca. 8,7 % (confusion matrix). 
The error of the old unoptimzed model is ca. 8,9 %. So the difference is only 0,2 %.


## H2O Performance

### Visualize the trade of between the precision and the recall and the optimal threshold

```{r, eval=FALSE}

#load a few models

stacked_ensemble_h2o <- h2o.loadModel("/Users/Amin/Documents/GitHub/ss23-bdml-aminraza/content/01_journal/h2o_models_new/StackedEnsemble_BestOfFamily_2_AutoML_7_20230613_130012")

gbm_h2o <- h2o.loadModel("/Users/Amin/Documents/GitHub/ss23-bdml-aminraza/content/01_journal/h2o_models_new/GBM_4_AutoML_7_20230613_130012")

glm_h2o <- h2o.loadModel("/Users/Amin/Documents/GitHub/ss23-bdml-aminraza/content/01_journal/h2o_models_new/GLM_1_AutoML_7_20230613_130012")

#Create a performance object
performance_h2o <- h2o.performance(stacked_ensemble_h2o, newdata = as.h2o(test_tbl))

typeof(performance_h2o)
#S4
performance_h2o %>% slotNames()

# We are focusing on the slot metrics. This slot contains all possible metrics
performance_h2o@metrics

# Classifier Summary Metrics

h2o.auc(performance_h2o, train = T, valid = T, xval = T)

```

```
train       valid       xval
0.9517821   0.9042058   0.9170230
```

```{r, eval=FALSE}

# Caution: "train, "val", and "xval" arugments only work for models (not performance objects)
h2o.auc(stacked_ensemble_h2o, train = T, valid = T, xval = T)

```

```
[1] 0.9514619
```

```{r, eval=FALSE}

h2o.giniCoef(performance_h2o)

h2o.logloss(performance_h2o)


# result for the training data
h2o.confusionMatrix(stacked_ensemble_h2o)


# result for the hold out set
h2o.confusionMatrix(performance_h2o)

# Precision vs Recall Plot

# This is on the test set
performance_tbl <- performance_h2o %>%
  h2o.metric() %>%
  as_tibble() 

performance_tbl %>% 
  glimpse()

theme_new <- theme(
  legend.position  = "bottom",
  legend.key       = element_blank(),,
  panel.background = element_rect(fill   = "transparent"),
  panel.border     = element_rect(color = "black", fill = NA, size = 0.5),
  panel.grid.major = element_line(color = "grey", size = 0.333)
) 

performance_tbl %>%
  filter(f1 == max(f1))

performance_tbl %>%
  ggplot(aes(x = threshold)) +
  geom_line(aes(y = precision), color = "blue", size = 1) +
  geom_line(aes(y = recall), color = "red", size = 1) +
  
  # Insert line where precision and recall are harmonically optimized
  geom_vline(xintercept = h2o.find_threshold_by_max_metric(performance_h2o, "f1")) +
  labs(title = "Precision vs Recall", y = "value") +
  theme_new

```

![Precesion vs Recall Plot](04_performance_measures_files/precisionrecall.jpg)


## ROC Plot

```{r, eval= FALSE}

# ROC Plot

path <- "h2o_models_new/GLM_1_AutoML_9_20230611_174526"

load_model_performance_metrics <- function(path, test_tbl) {
  
  model_h2o <- h2o.loadModel(path)
  perf_h2o  <- h2o.performance(model_h2o, newdata = as.h2o(test_tbl)) 
  
  perf_h2o %>%
    h2o.metric() %>%
    as_tibble() %>%
    mutate(auc = h2o.auc(perf_h2o)) %>%
    select(tpr, fpr, auc)
  
}

model_metrics_tbl <- fs::dir_info(path = "h2o_models_new") %>%
  select(path) %>%
  mutate(metrics = map(path, load_model_performance_metrics, test_tbl)) %>%
  unnest(cols = metrics)


model_metrics_tbl %>%
  mutate(
    # Extract the model names
    path = str_split(path, pattern = "/", simplify = T)[,2] %>% as_factor(),
    auc  = auc %>% round(3) %>% as.character() %>% as_factor()
  ) %>%
  ggplot(aes(fpr, tpr, color = path, linetype = auc)) +
  geom_line(size = 1) +
  
  # just for demonstration purposes
  geom_abline(color = "red", linetype = "dotted") +
  
  theme_new +
  theme(
    legend.direction = "vertical",
  ) +
  labs(
    title = "ROC Plot",
    subtitle = "Performance of 3 Top Performing Models"
  )


```

![ROC Plot](04_performance_measures_files/ROC_Plot.jpg)

You can clearly see, that the GLM Model performs the worst. This no surprise since it was the very last model on the leaderboard. The difference on the other two models however is close to zero. 

## Precision vs Recall Plot

```{r, eval=FALSE}

# Precision vs Recall

load_model_performance_metrics2 <- function(path, test_tbl) {
  
  model_h2o <- h2o.loadModel(path)
  perf_h2o  <- h2o.performance(model_h2o, newdata = as.h2o(test_tbl)) 
  
  perf_h2o %>%
    h2o.metric() %>%
    as_tibble() %>%
    mutate(auc = h2o.auc(perf_h2o)) %>%
    select(tpr, fpr, auc, precision, recall)
  
}

model_metrics_tbl_2 <- fs::dir_info(path = "h2o_models_new") %>%
  select(path) %>%
  mutate(metrics = map(path, load_model_performance_metrics2, test_tbl)) %>%
  unnest(cols = metrics)

model_metrics_tbl_2 %>%
  mutate(
    path = str_split(path, pattern = "/", simplify = T)[,2] %>% as_factor(),
    auc  = auc %>% round(3) %>% as.character() %>% as_factor()
  ) %>%
  ggplot(aes(recall, precision, color = path, linetype = auc)) +
  geom_line(size = 1) +
  theme_new + 
  theme(
    legend.direction = "vertical",
  ) +
  labs(
    title = "Precision vs Recall Plot",
    subtitle = "Performance of 3 Top Performing Models"
  )

```

![ROC Plot](04_performance_measures_files/Precision_vs_Recall_Plot2.jpg)

Same here. GLM in comparison performs the worst.

## Gain & Lift

```{r, eval=FALSE}

# Gain & Lift

#Because we need our predictions again we calculate a prediction of the best performing model we have in this session
#Best Performing Model: stacked ensenmble

predictions <- h2o.predict(stacked_ensemble_h2o, newdata = as.h2o(test_tbl))

predictions_tbl <- predictions %>% as_tibble()

ranked_predictions_tbl <- predictions_tbl %>%
  bind_cols(test_tbl) %>%
  select(predict:Yes, went_on_backorder) %>%
  # Sorting from highest to lowest class probability
  arrange(desc(Yes))

```

```{r}
read_csv("04_performance_measures_files/ranked_predictions_tbl.csv")
```

```{r, eval=FALSE}
ranked_predictions_tbl %>%
  mutate(ntile = ntile(Yes, n = 10)) %>%
  group_by(ntile) %>%
  summarise(
    cases = n(),
    responses = sum(went_on_backorder == "Yes")
  ) %>%
  arrange(desc(ntile))

```

```{r}
read_csv("04_performance_measures_files/ranked_predictions_step2.csv")
```

Look at the 10 ntile: 213 of 285 went actually on backorder.

```{r, eval=FALSE}

calculated_gain_lift_tbl <- ranked_predictions_tbl %>%
  mutate(ntile = ntile(Yes, n = 10)) %>%
  group_by(ntile) %>%
  summarise(
    cases = n(),
    responses = sum(went_on_backorder == "Yes")
  ) %>%
  arrange(desc(ntile)) %>%
  
  # Add group numbers (opposite of ntile)
  mutate(group = row_number()) %>%
  select(group, cases, responses) %>%
  
  # Calculations
  mutate(
    cumulative_responses = cumsum(responses),
    pct_responses        = responses / sum(responses),
    gain                 = cumsum(pct_responses),
    cumulative_pct_cases = cumsum(cases) / sum(cases),
    lift                 = gain / cumulative_pct_cases,
    gain_baseline        = cumulative_pct_cases,
    lift_baseline        = gain_baseline / cumulative_pct_cases
  )

calculated_gain_lift_tbl 

```

```{r}
read_csv("04_performance_measures_files/calculated_gain_lift_tbl.csv")
```

```{r, eval=FALSE}
#Calculating Gain & Lift manually was rather for understanding the concept.
#Of course H2O can do the calculation for us.

gain_lift_tbl <- performance_h2o %>%
  h2o.gainsLift() %>%
  as.tibble()

## Gain Chart

gain_transformed_tbl <- gain_lift_tbl %>% 
  select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift) %>%
  select(-contains("lift")) %>%
  mutate(baseline = cumulative_data_fraction) %>%
  rename(gain     = cumulative_capture_rate) %>%
  # prepare the data for the plotting (for the color and group aesthetics)
  pivot_longer(cols = c(gain, baseline), values_to = "value", names_to = "key")

gain_transformed_tbl %>%
  ggplot(aes(x = cumulative_data_fraction, y = value, color = key)) +
  geom_line(size = 1.5) +
  labs(
    title = "Gain Chart",
    x = "Cumulative Data Fraction",
    y = "Gain"
  ) +
  theme_new
```

### Gain Plot

![Gain Plot](04_performance_measures_files/Gain_Chart.jpg)

```{r, eval=FALSE}
## Lift Plot

lift_transformed_tbl <- gain_lift_tbl %>% 
  select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift) %>%
  select(-contains("capture")) %>%
  mutate(baseline = 1) %>%
  rename(lift = cumulative_lift) %>%
  pivot_longer(cols = c(lift, baseline), values_to = "value", names_to = "key")

lift_transformed_tbl %>%
  ggplot(aes(x = cumulative_data_fraction, y = value, color = key)) +
  geom_line(size = 1.5) +
  labs(
    title = "Lift Chart",
    x = "Cumulative Data Fraction",
    y = "Lift"
  ) +
  theme_new
```

### Lift Plot

![Gain Plot](04_performance_measures_files/Lift_Chart.jpg)

## Dashboard with cowplot

Here comes a bunch of code I copied from the task description/tutorial. Of course I customized it with respect to my needs.

```{r, eval=FALSE}
# 5. Performance Visualization ----  
library(cowplot)
library(glue)


# set values to test the function while building it
h2o_leaderboard <- automl_models_h2o@leaderboard
newdata <- test_tbl
order_by <- "auc"
max_models <- 4
size <- 1

plot_h2o_performance <- function(h2o_leaderboard, newdata, order_by = c("auc", "logloss"),
                                 max_models = 3, size = 1.5) {
  
  # Inputs
  
  leaderboard_tbl <- h2o_leaderboard %>%
    as_tibble() %>%
    slice(1:max_models)
  
  newdata_tbl <- newdata %>%
    as_tibble()
  
  # Selecting the first, if nothing is provided
  order_by      <- tolower(order_by[[1]]) 
  
  # Convert string stored in a variable to column name (symbol)
  order_by_expr <- rlang::sym(order_by)
  
  # Turn of the progress bars ( opposite h2o.show_progress())
  h2o.no_progress()
  
  # 1. Model metrics
  
  get_model_performance_metrics <- function(model_id, test_tbl) {
    
    model_h2o <- h2o.getModel(model_id)
    perf_h2o  <- h2o.performance(model_h2o, newdata = as.h2o(test_tbl))
    
    perf_h2o %>%
      h2o.metric() %>%
      as.tibble() %>%
      select(threshold, tpr, fpr, precision, recall)
    
  }
  
  model_metrics_tbl <- leaderboard_tbl %>%
    mutate(metrics = map(model_id, get_model_performance_metrics, newdata_tbl)) %>%
    unnest(cols = metrics) %>%
    mutate(
      model_id = as_factor(model_id) %>% 
        # programmatically reorder factors depending on order_by
        fct_reorder(!! order_by_expr, 
                    .desc = ifelse(order_by == "auc", TRUE, FALSE)),
      auc      = auc %>% 
        round(3) %>% 
        as.character() %>% 
        as_factor() %>% 
        fct_reorder(as.numeric(model_id)),
      logloss  = logloss %>% 
        round(4) %>% 
        as.character() %>% 
        as_factor() %>% 
        fct_reorder(as.numeric(model_id))
    )
  
  
  # 1A. ROC Plot
  
  p1 <- model_metrics_tbl %>%
    ggplot(aes(fpr, tpr, color = model_id, linetype = !! order_by_expr)) +
    geom_line(size = size) +
    theme_new +
    labs(title = "ROC", x = "FPR", y = "TPR") +
    theme(legend.direction = "vertical") 
  
  
  # 1B. Precision vs Recall
  
  p2 <- model_metrics_tbl %>%
    ggplot(aes(recall, precision, color = model_id, linetype = !! order_by_expr)) +
    geom_line(size = size) +
    theme_new +
    labs(title = "Precision Vs Recall", x = "Recall", y = "Precision") +
    theme(legend.position = "none") 
  
  
  # 2. Gain / Lift
  
  get_gain_lift <- function(model_id, test_tbl) {
    
    model_h2o <- h2o.getModel(model_id)
    perf_h2o  <- h2o.performance(model_h2o, newdata = as.h2o(test_tbl)) 
    
    perf_h2o %>%
      h2o.gainsLift() %>%
      as.tibble() %>%
      select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift)
    
  }
  
  gain_lift_tbl <- leaderboard_tbl %>%
    mutate(metrics = map(model_id, get_gain_lift, newdata_tbl)) %>%
    unnest(cols = metrics) %>%
    mutate(
      model_id = as_factor(model_id) %>% 
        fct_reorder(!! order_by_expr, 
                    .desc = ifelse(order_by == "auc", TRUE, FALSE)),
      auc  = auc %>% 
        round(3) %>% 
        as.character() %>% 
        as_factor() %>% 
        fct_reorder(as.numeric(model_id)),
      logloss = logloss %>% 
        round(4) %>% 
        as.character() %>% 
        as_factor() %>% 
        fct_reorder(as.numeric(model_id))
    ) %>%
    rename(
      gain = cumulative_capture_rate,
      lift = cumulative_lift
    ) 
  
  # 2A. Gain Plot
  
  p3 <- gain_lift_tbl %>%
    ggplot(aes(cumulative_data_fraction, gain, 
               color = model_id, linetype = !! order_by_expr)) +
    geom_line(size = size,) +
    geom_segment(x = 0, y = 0, xend = 1, yend = 1, 
                 color = "red", size = size, linetype = "dotted") +
    theme_new +
    expand_limits(x = c(0, 1), y = c(0, 1)) +
    labs(title = "Gain",
         x = "Cumulative Data Fraction", y = "Gain") +
    theme(legend.position = "none")
  
  # 2B. Lift Plot
  
  p4 <- gain_lift_tbl %>%
    ggplot(aes(cumulative_data_fraction, lift, 
               color = model_id, linetype = !! order_by_expr)) +
    geom_line(size = size) +
    geom_segment(x = 0, y = 1, xend = 1, yend = 1, 
                 color = "red", size = size, linetype = "dotted") +
    theme_new +
    expand_limits(x = c(0, 1), y = c(0, 1)) +
    labs(title = "Lift",
         x = "Cumulative Data Fraction", y = "Lift") +
    theme(legend.position = "none") 
  
  
  # Combine using cowplot
  
  # cowplot::get_legend extracts a legend from a ggplot object
  p_legend <- get_legend(p1)
  # Remove legend from p1
  p1 <- p1 + theme(legend.position = "none")
  
  # cowplot::plt_grid() combines multiple ggplots into a single cowplot object
  p <- cowplot::plot_grid(p1, p2, p3, p4, ncol = 2)
  
  # cowplot::ggdraw() sets up a drawing layer
  p_title <- ggdraw() + 
    
    # cowplot::draw_label() draws text on a ggdraw layer / ggplot object
    draw_label("H2O Model Metrics", size = 18, fontface = "bold", 
               color = "#2C3E50")
  
  p_subtitle <- ggdraw() + 
    draw_label(glue("Ordered by {toupper(order_by)}"), size = 10,  
               color = "#2C3E50")
  
  # Combine everything
  ret <- plot_grid(p_title, p_subtitle, p, p_legend, 
                   
                   # Adjust the relative spacing, so that the legends always fits
                   ncol = 1, rel_heights = c(0.05, 0.05, 1, 0.05 * max_models))
  
  h2o.show_progress()
  
  return(ret)
  
}

automl_models_h2o@leaderboard %>%
  plot_h2o_performance(newdata = test_tbl, order_by = "logloss", 
                       size = 0.5, max_models = 4)
```

And this the beatiful plot we get:

![H2O Model Metrics](04_performance_measures_files/h2o_model_metrics.jpg)
